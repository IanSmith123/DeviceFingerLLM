{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d90baf9-ce33-4d65-b3aa-5b5d214abb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fbfb2f6-6714-47a8-b69d-e873a24fd412",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/binhaoyu/miniconda3/envs/unsloth/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "==((====))==  Unsloth 2024.8: Fast Llama patching. Transformers = 4.44.0.\n",
      "   \\\\   /|    GPU: Tesla V100-PCIE-32GB. Max memory: 31.739 GB. Platform = Linux.\n",
      "O^O/ \\_/ \\    Pytorch: 2.3.0. CUDA = 7.0. CUDA Toolkit = 12.1.\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.26.post1. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.8 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaExtendedRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset\n",
    "# use bert to encode text\n",
    "import json\n",
    "import dataclasses\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import tqdm\n",
    "from IPython.core.usage import default_banner\n",
    "\n",
    "from transformers import TextStreamer\n",
    "from unsloth import FastLanguageModel\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "max_seq_length = 2048  # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None  # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True  # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config:\n",
    "    path_uid_output = Path('uid_candidate_with_banner')\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class UID:\n",
    "    banner: str\n",
    "    unique_id: str\n",
    "    ip: str\n",
    "    port: int\n",
    "    timestamp: str\n",
    "    service_name: str\n",
    "    rule_name: str\n",
    "    req_data: str\n",
    "    vendor: str\n",
    "    product: str\n",
    "    model: str\n",
    "    uid_predict: Optional[str] = None\n",
    "    bert_encoding: Optional[List[torch.FloatTensor]] = None\n",
    "    cluster_label: Optional[str] = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"{self.cluster_label}:{self.rule_name}:{self.ip}:{self.port}:{self.timestamp}\"\n",
    "\n",
    "\n",
    "def get_file_data(filename: Path) -> List[UID]:\n",
    "    file_content = [UID(**json.loads(line)) for line in filename.read_text().splitlines()]\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def get_tokenizer_and_model():\n",
    "    _model, _tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
    "        max_seq_length=max_seq_length,\n",
    "        dtype=dtype,\n",
    "        load_in_4bit=load_in_4bit,\n",
    "    )\n",
    "    FastLanguageModel.for_inference(_model)  # Enable native 2x faster inference\n",
    "    return _tokenizer, _model\n",
    "\n",
    "model_name = \"unsloth/Meta-Llama-3.1-8B\"\n",
    "model_name = \"unsloth/mistral-7b-v0.3-bnb-4bit\"         # Mistral v3 2x faster!\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model_name = \"Mistral-Nemo-Instruct-2407-bnb-4bit\"\n",
    "model_name = \"Meta-Llama-3.1-8B\"\n",
    "model_name = \"gemma-2-9b-bnb-4bit\"\n",
    "model_name = \"mistral-7b-v0.3-bnb-4bit\"\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct-bnb-4bit\"\n",
    "model_name = \"Mistral-Nemo-Base-2407-bnb-4bit\" # New Mistral 12b 2x faster!\n",
    "model_name = \"Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "\n",
    " \n",
    "def get_raw_tokenizer_and_model():\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = f\"unsloth/{model_name}\" ,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )     \n",
    "    FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n",
    "\n",
    "    return tokenizer, model\n",
    "\n",
    "\n",
    "def predict_streamer(banner: str, tokenizer, model):\n",
    "    question = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {''' You are an expert in data mining and network security. I will provide you with some network scan data, which is divided by lines, with each line representing a piece of data. Not every record can successfully extract a fingerprint; you only need to find the data that you believe can serve as a fingerprint. You need to fully understand the semantics of this data, determine the meaning of the string or the strings that follow, and utilize your knowledge of web development, such as the Date field in the HTTP header, nonce in requests, and csrf_token that frequently change. Filter out data that frequently changes and cannot be used as a fingerprint, and then identify the specific strings that meet the following requirements for a fingerprint:\n",
    "        The fingerprint must uniquely identify an individual device, not just a type of device.\n",
    "        The fingerprint must not change upon repeated network requests.\n",
    "        The fingerprint must remain consistent over a long period.\n",
    "        The fingerprint must not change upon device reboot.\n",
    "        '''}\n",
    "\n",
    "    ### Input:\n",
    "    {banner}\n",
    "\n",
    "    ### Response:\n",
    "    {''}\"\"\"\n",
    "\n",
    "    inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    # output to stdout\n",
    "    model.generate(**inputs, streamer=text_streamer, max_new_tokens=128)\n",
    "    # a = model.pred(**inputs)\n",
    "    # print(a)\n",
    "    # print(pred)\n",
    "\n",
    "\n",
    "def predict_direct(banner: str, tokenizer, model):\n",
    "    question = f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {''' You are an expert in data mining and network security. I will provide you with some network scan data, which is divided by lines, with each line representing a piece of data. Not every record can successfully extract a fingerprint; you only need to find the data that you believe can serve as a fingerprint. You need to fully understand the semantics of this data, determine the meaning of the string or the strings that follow, and utilize your knowledge of web development, such as the Date field in the HTTP header, nonce in requests, and csrf_token that frequently change. Filter out data that frequently changes and cannot be used as a fingerprint, and then identify the specific strings that meet the following requirements for a fingerprint:\n",
    "        The fingerprint must uniquely identify an individual device, not just a type of device.\n",
    "        The fingerprint must not change upon repeated network requests.\n",
    "        The fingerprint must remain consistent over a long period.\n",
    "        The fingerprint must not change upon device reboot.\n",
    "        '''}\n",
    "\n",
    "    ### Input:\n",
    "    {banner}\n",
    "\n",
    "    ### Response:\n",
    "    {''}\"\"\"\n",
    "\n",
    "    inputs = tokenizer([question], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # 解码模型生成的 tokens\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return decoded_output\n",
    "\n",
    "\n",
    "# batch pred\n",
    "def predict_direct_batch(banner_list: List[str], tokenizer, model):\n",
    "    questions = [f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction:\n",
    "    {''' You are an expert in data mining and network security. I will provide you with some network scan data, which is divided by lines, with each line representing a piece of data. Not every record can successfully extract a fingerprint; you only need to find the data that you believe can serve as a fingerprint. You need to fully understand the semantics of this data, determine the meaning of the string or the strings that follow, and utilize your knowledge of web development, such as the Date field in the HTTP header, nonce in requests, and csrf_token that frequently change. Filter out data that frequently changes and cannot be used as a fingerprint, and then identify the specific strings that meet the following requirements for a fingerprint:\n",
    "        The fingerprint must uniquely identify an individual device, not just a type of device.\n",
    "        The fingerprint must not change upon repeated network requests.\n",
    "        The fingerprint must remain consistent over a long period.\n",
    "        The fingerprint must not change upon device reboot.\n",
    "        '''}\n",
    "\n",
    "    ### Input:\n",
    "    {banner}\n",
    "\n",
    "    ### Response:\n",
    "    {''}\"\"\" for banner in banner_list]\n",
    "    # print(\"q\", \"*\"*30, questions)\n",
    "\n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "    # text_streamer = TextStreamer(tokenizer)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # 解码模型生成的 tokens\n",
    "    decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return decoded_outputs\n",
    "\n",
    "\n",
    "llama_tokenizer, llama_model = get_tokenizer_and_model()\n",
    "# llama_tokenizer, llama_model = get_raw_tokenizer_and_model()\n",
    "llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ea53e5c-5fb8-4e99-968e-1e3965c3849d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid_candidate_with_banner/ftp.jsonl 299591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                                                                          | 509/299591 [00:44<7:17:10, 11.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid_candidate_with_banner/rtsp.jsonl 7520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█████▎                                                                         | 509/7520 [00:46<10:40, 10.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uid_candidate_with_banner/onvif.jsonl 3118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▉                                                                  | 509/3118 [03:00<15:24,  2.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "\n",
    "model_name = f\"500-{model_name}\"\n",
    "all_result = defaultdict(list)\n",
    "for uid_file in Config.path_uid_output.iterdir():\n",
    "    result = []\n",
    "    records = get_file_data(uid_file)\n",
    "    print(uid_file, len(records))\n",
    "    to_pred = []\n",
    "    for uid_obj in tqdm.tqdm(records):\n",
    "        resp_banner = uid_obj.banner\n",
    "        # print(resp_banner)\n",
    "        to_pred.append(uid_obj)\n",
    "        if len(to_pred) == 15:\n",
    "            aa = predict_direct_batch([i.banner for i in to_pred], tokenizer=llama_tokenizer, model=llama_model)\n",
    "            for i in range(len(aa)):\n",
    "                to_pred[i].uid_predict = aa[i]\n",
    "            all_result[uid_file.name].extend(to_pred)\n",
    "            to_pred = []\n",
    "\n",
    "        if len(all_result[uid_file.name]) > 50 * 10:\n",
    "            break\n",
    "\n",
    "for filename in all_result:\n",
    "    filepath = Path(model_name) / filename\n",
    "    filepath.parent.mkdir(exist_ok=True)\n",
    "    filepath.write_text(\"\\n\".join([json.dumps( dataclasses.asdict(i)) for i in all_result[filename]]))\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab3b5e-2376-48c3-b97a-e43e60f60617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1a7fb6-8f60-4baa-a5d9-3c2d41383680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
